<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Django | Pritish Chakraborty]]></title>
  <link href="http://PritishC.github.io/blog/categories/django/atom.xml" rel="self"/>
  <link href="http://PritishC.github.io/"/>
  <updated>2016-02-14T21:01:33+05:30</updated>
  <id>http://PritishC.github.io/</id>
  <author>
    <name><![CDATA[Pritish Chakraborty]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Uploading With Django and Amazon S3]]></title>
    <link href="http://PritishC.github.io/blog/2015/09/06/uploading-with-django-and-amazon-s3/"/>
    <updated>2015-09-06T17:20:49+05:30</updated>
    <id>http://PritishC.github.io/blog/2015/09/06/uploading-with-django-and-amazon-s3</id>
    <content type="html"><![CDATA[In this short post, I describe how I configured Django to upload to
Amazon S3 instead of a regular file-system upload. It can be useful
for production scenarios.

<!--more-->

We will be using the `django-storages` package to make it easier to
upload to S3. It is always better not to reinvent the wheel, but if
you&#8217;d like to explore on how to do it on your own, you definitely
should!

Install via pip.

&#8220;` bash django-storages
$ pip install django-storages
$ pip freeze | grep django-storages >> requirements.txt
&#8220;`

I&#8217;m assuming that you already have a virtual environment set up for your
django project, and are installing within it.

Now in my case, I&#8217;ve deployed my webapp to both Heroku and [AWS]({% post_url 2015-09-03-docker-is-awesome %}).
I have different settings files for both, and I wanted my S3 setup to
be respected on both configurations. To this end, we create a separate
settings file which holds the S3 settings.

&#8220;` python aws_settings.py
import os

AWS_QUERYSTRING_AUTH = False
AWS_ACCESS_KEY_ID = os.environ[&#8216;AWS_ACCESS_KEY&#8217;]
AWS_SECRET_ACCESS_KEY = os.environ[&#8216;AWS_SECRET_ACCESS_KEY&#8217;]
AWS_STORAGE_BUCKET_NAME = os.environ[&#8216;S3_BUCKET_NAME&#8217;]
MEDIA_URL = &#8216;http://%s.s3.amazonaws.com/your-folder/&#8217; % AWS_STORAGE_BUCKET_NAME
DEFAULT_FILE_STORAGE = &#8220;storages.backends.s3boto.S3BotoStorage&#8221;
&#8220;`

In your production settings file, at the bottom, add an `if` check for
production and import these settings. I do it by looking for a particular
environment variable which is set only on production.

&#8220;` python prod.py
&#8230;
&#8230;

if os.environ.get(&#8216;ENV_VAR&#8217;) == &#8216;prod&#8217;:
    from aws_settings import *
&#8220;`

On setting `DEFAULT_FILE_STORAGE` to `S3BotoStorage`, django-storages requires
the set of AWS credentials which you plan to use to upload to an S3 bucket.
If you don&#8217;t already have them, you can go to the IAM console on AWS
and generate them. These are the permissions I added to the security
group which I applied on these credentials -:

{% img /images/AWS_IAM_S3.png &#8216;S3 Credentials Policies&#8217; %}

The primary purpose of these credentials is to allow (in my case) an
admin user to upload/delete images on an S3 bucket. We will let the
public view images, but not manipulate them in any other way, nor abuse
the system. Note the IAM ID of the credentials, as you will need it later.

{% img /images/AWS_IAM_ID.png &#8216;IAM User ARN or ID&#8217; %}

Now head to the S3 management console on AWS and create a bucket.

You will be presented with a prompt to enter the bucket name and the region
where it should be deployed. Choose a region keeping in mind your target
audience. You are also allowed to set up logging, with a prefix which is
basically a &#8216;folder&#8217; in which log files will be stored. (It is easier to
think of it as a folder, though that is not entirely the case)

{% img /images/AWS_S3_BUCKET_1.png &#8216;Create A Bucket&#8217; %}

{% img /images/AWS_S3_BUCKET_2.png &#8216;Logging Buckets&#8217; %}

Once the bucket has been created, we have to configure certain permissions
on it. This is what the Properties section of a newly created bucket
looks like -:

{% img /images/AWS_BUCKET_PROPS.png &#8216;Bucket Permissions&#8217; %}

Let&#8217;s add a permission policy to our bucket. Click on &#8216;Edit bucket policy&#8217;
and paste the following -:

&#8220;` json bucket policy
{
	&#8220;Version&#8221;: &#8220;2008-10-17&#8221;,
	&#8220;Statement&#8221;: [
		{
			&#8220;Sid&#8221;: &#8220;&#8221;,
			&#8220;Effect&#8221;: &#8220;Allow&#8221;,
			&#8220;Principal&#8221;: {
				&#8220;AWS&#8221;: &#8220;*&#8221;
			},
			&#8220;Action&#8221;: &#8220;s3:GetObject&#8221;,
			&#8220;Resource&#8221;: [
				&#8220;arn:aws:s3:::testbucket/*&#8221;,
				&#8220;arn:aws:s3:::testbucket&#8221;
			]
		},
		{
			&#8220;Sid&#8221;: &#8220;&#8221;,
			&#8220;Effect&#8221;: &#8220;Allow&#8221;,
			&#8220;Principal&#8221;: {
				&#8220;AWS&#8221;: &#8220;arn:aws:iam::ID_NUMBER_HERE:root&#8221;
			},
			&#8220;Action&#8221;: &#8220;s3:*&#8221;,
			&#8220;Resource&#8221;: [
				&#8220;arn:aws:s3:::testbucket/*&#8221;,
				&#8220;arn:aws:s3:::testbucket&#8221;
			]
		}
	]
}
&#8220;`

This is, in effect, a combination of policies. The first part of the policy
enforces public-read, i.e., anyone can read data on the bucket. The second
part of the policy allows any action to be performed (get, put, delete),
but this is restricted to the user with the IAM ID as given. Paste the IAM
ARN/ID from earlier here, and hit Save.

In my case, I had to edit the CORS configuration, though this may not be
necessary for you. On the same Properties > Permissions section, hit the
&#8216;Edit CORS Configuration&#8217; button and paste the following -:

&#8220;` xml cors config
<?xml version="1.0" encoding="UTF-8"?>
<CORSConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">
    <CORSRule>
        <AllowedOrigin>*</AllowedOrigin>
        <AllowedMethod>GET</AllowedMethod>
        <AllowedMethod>POST</AllowedMethod>
        <MaxAgeSeconds>3000</MaxAgeSeconds>
        <AllowedHeader>Authorization</AllowedHeader>
    </CORSRule>
</CORSConfiguration>
&#8220;`

This allows users from other domains to make HTTP requests (GET and POST) on
our bucket.

And there you have it. Now you can upload files to S3, and view them in
your bucket. There should be an `uploads` folder (standard django stuff)
in the bucket after your first upload.

<div id="discourse-comments"></div>
<script type="text/javascript">
  var discourseUrl = "",
      discourseEmbedUrl = "http://PritishC.github.io/blog/2015/09/06/uploading-with-django-and-amazon-s3/";

  (function() {
    var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
      d.src = discourseUrl + 'javascripts/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
  })();
</script>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Is Awesome - Part I]]></title>
    <link href="http://PritishC.github.io/blog/2015/09/03/docker-is-awesome/"/>
    <updated>2015-09-03T21:56:16+05:30</updated>
    <id>http://PritishC.github.io/blog/2015/09/03/docker-is-awesome</id>
    <content type="html"><![CDATA[It's been a while since I've written a blog post. There's been lots of changes that I've
had to deal with, the big one being a change of workplace. And I realize that this post
should've been the one explaining my adventures with Elasticsearch. That will have to wait
for a bit, my apologies!

In this article, I'm going to explain my (possibly flawed) method of deploying a Django/Angular
app using Docker. I've wanted to learn how to deploy an app to AWS for a while, and Docker
helped me do just that.

<!--more-->

I assume that the reader has a basic knowledge of the docker toolbox; if not, I&#8217;ll explain them in brief in
part 2. To begin with, here&#8217;s the directory structure of my app for reference (generated using the
lovely `tree` command on Linux) -:

&#8220;` bash app structure
|&#8211; bower.json
|&#8211; CONTRIBUTORS
|&#8211; project
|   |&#8211; project
|   |   |&#8211; aws_settings.py (django-storages credentials here)
|   |   |&#8211; heroku_settings.py
|   |   |&#8211; __init__.py
|   |   |&#8211; settings.py
|   |   |&#8211; urls.py
|   |   |&#8211; views.py
|   |   `&#8211; wsgi.py
|   |&#8211; manage.py
|   |&#8211; app1
|   |   |&#8211; __init__.py
|   |   |&#8211; permissions.py
|   |   |&#8211; serializers.py
|   |   |&#8211; services.py
|   |   |&#8211; tests.py
|   |   `&#8211; views.py
|   `&#8211; users
|       |&#8211; __init__.py
|       |&#8211; models.py
|       |&#8211; permissions.py
|       |&#8211; serializers.py
|       `&#8211; views.py
|&#8211; gulpfile.js
|&#8211; package.json
|&#8211; Procfile
|&#8211; README.md
|&#8211; requirements.txt
|&#8211; scripts
|   `&#8211; postInstall.sh
|&#8211; static
|   |&#8211; javascripts
|   |   |&#8211; app.js
|   |   |&#8211; controllers
|   |   |   `&#8211; controllers.js
|   |   |&#8211; directives
|   |   |   `&#8211; directives.js
|   |   `&#8211; services
|   |       `&#8211; services.js
|   |&#8211; partials (angular view stuff)
|   `&#8211; stylesheets
|       `&#8211; styles.css
|&#8211; templates
|   |&#8211; index.html
|   |&#8211; javascripts.html
|   |&#8211; navbar.html
|   `&#8211; stylesheets.html
&#8220;`

This follows from the nice boilerplate provided by [thinkster.io](https://github.com/brwr/thinkster-django-angular-boilerplate) - take a look at their nifty tutorials [here](http://thinkster.io).

I first stumbled across this article on [realpython.com](https://realpython.com/blog/python/django-development-with-docker-compose-and-machine/), which greatly piqued my interest. I realized that docker had
moved on to becoming a suite of tools - docker itself becoming docker-engine in name. However,
I faced issues (being a total n00b in devops) in setting up with the configuration that they
specified, so I decided to go for something simpler. I found just what I needed in [Andre&#8217;s](https://github.com/andrecp/django-tutorial-docker-nginx-postgres)
tutorial for deploying a simple Docker-Nginx-Django-Postgres setup.

One thing I noticed in both configurations is that the code repository was bundled alongwith the docker
stuff (needed by docker-compose) for production.
I couldn&#8217;t agree with that, so I decided to look up a method to clone my repository while creating
the docker container. I then kept my code and deployment repositories separate, and found that Bitbucket
(where my repositories are hosted) have a feature called deployment keys - SSH keys that have read-only
access to a repository. This was exactly what I needed.

Here is the directory structure of my docker deployment repository -:

&#8220;` bash docker directories
|&#8211; docker-compose.yml
|&#8211; Dockerfile
|&#8211; ec2box.sh
|&#8211; nginx
|   |&#8211; container_ip.sh
|   |&#8211; Dockerfile
|   `&#8211; nginx.conf
|&#8211; README.md
|&#8211; rebuild_docker.sh
|&#8211; ssh
|   |&#8211; bb_deploy.rsa
|   |&#8211; bb_deploy.rsa.pub
|   `&#8211; config
`&#8211; static
    `&#8211; admin
&#8220;`

* docker-compose.yml : The file that controls how docker-compose builds docker containers and runs them
* Dockerfile : The Dockerfile for the django/angular container
* ec2box.sh : A small script containing a single command which creates the whole setup using AWS drivers
* nginx - Directory containing specifics for the nginx container, where container_ip.sh is another
small script which I needed when deploying on AWS
* rebuild_docker.sh : A script from Andre&#8217;s repository for quickly build-and-up containers using docker-compose
* ssh : Directory containing my Bitbucket deployment key and a SSH config file
* static : Django admin static files

The contents of docker-compose.yml are as follows -:

&#8220;` yaml docker-compose.yml
# Nginx
nginx:
    build: ./nginx
    volumes_from:
        - django
    links:
        - django
    ports:
        - &#8220;80:80&#8221;

# This defines a service for the Django app
# Will include the Angular frontend
django:
    build: .
    volumes:
        - .:/root
        - /usr/src/app
    expose:
        - &#8220;8000&#8221;
    links:
        - postgres

# This defines a service for the Postgres database
postgres:
    image: postgres:latest
&#8220;`

Note how the nginx container definition specifies a `volumes_from` section, of which the django
container is a part. The host port 80 has been mapped to container port 80, as nginx requires.
One little caveat: make sure that no other docker containers are hogging up port 80, because you
will have a painful time trying to find out why your nginx container keeps dying on you. The `links`
directive creates entries in the nginx container&#8217;s `/etc/hosts` file for the django container&#8217;s IP/hostname.
This will come in use later when we deploy to AWS.

The django container definition has a few small differences from the one mentioned at realpython or
Andre&#8217;s tutorial. We mount the volume on `/root` instead of `/usr/src/app`, because the latter does
not exist until we clone the code repository. Additionally, we expose `/usr/src/app` as a volume, so
that the nginx container does not run into a load of 404s when trying to serve static files. Port 8000
is exposed as we shall be running gunicorn on that port, and there is a link to the postgres container.

The postgres definition is not much to talk about, as it is built from an image from the docker registry
(yes, they have a registry of known docker containers!).


Let&#8217;s take a look at the Dockerfile for the django container -:

&#8220;` sh Dockerfile (django)
FROM ubuntu:14.04

ENV DJANGO_CONFIGURATION Docker

# First, we need to get git, and clone our repository
# Additionally, get everything else here too, such as nodejs and npm

RUN apt-get update
RUN apt-get install -y ca-certificates git-core ssh nodejs npm python-pip libpq-dev python-dev
RUN ln -s /usr/bin/nodejs /usr/bin/node

ENV HOME /root

# Add custom ssh keypair - usually Bitbucket deployment keys
ADD ssh/ /root/.ssh/

# Fix permissions
RUN chmod 600 /root/.ssh/*

# Avoid first connection host confirmation
RUN ssh-keyscan bitbucket.org > /root/.ssh/known_hosts

# Clone the repo
WORKDIR /usr/src/app
RUN git clone git@bitbucket.org:username/repo

# Install requirements
WORKDIR /usr/src/app/repo
RUN pip install -r requirements.txt
RUN npm install -g bower
RUN bower &#8211;allow-root install

# S3 Storage for django-storages
ENV AWS_ACCESS_KEY yourkeyhere
ENV AWS_SECRET_ACCESS_KEY yoursecretsaucehere
ENV S3_BUCKET_NAME yourbucketnamehere

# DB Settings
ENV DB_NAME postgres
ENV DB_USER postgres
ENV DB_PASS postgres
ENV DB_SERVICE postgres

# Add Django Admin CSS
ADD ./static/admin /usr/src/app/repo/static/admin

WORKDIR /usr/src/app/repo/defsec
CMD [&#8220;gunicorn&#8221;, &#8220;app.wsgi&#8221;, &#8220;-w&#8221;, &#8220;2&#8221;, &#8220;-b&#8221;, &#8220;0.0.0.0:8000&#8221;, &#8220;&#8211;log-level&#8221;, &#8220;-&#8220;]
&#8220;`

The `DJANGO_CONFIGURATION` environment variable is used in django project settings. First, we
install all the necessary command line tools, and create a symbolic link so that nodejs plays
well. We then add our custom keypair to the `.ssh` directory, and run a permissions fix command.
This will allow us to clone the code repository. Before running the clone command, we run a
`ssh-keyscan` so that the cloning process is automatic - no key passphrase prompts. Some may argue
that this lowers security, but that is a topic for another post altogether.
After cloning, I install all django/angular requirements using pip and bower. A few more environment
variables are then set, first for django-storages (a topic for a soon-to-come post: how I set up
image uploads to Amazon S3 with django-storages) and then for the postgres database. Finally,
django admin static files are added (yes, these don&#8217;t come out of nowhere, they need to be added
for nginx to serve them) and run the gunicorn server.

With that covered, let&#8217;s check out the contents of the nginx directory. First up is the nginx
Dockerfile -:

&#8220;` sh nginx Dockerfile
# Set nginx base image
FROM nginx

# File Author / Maintainer
MAINTAINER Pritish Chakraborty

# Copy custom configuration file from the current directory
COPY nginx.conf /etc/nginx/nginx.conf

# Uncomment the commented Dockerfile lines below when pushing to AWS
# COPY container_ip.sh /root/container_ip.sh

# Get django container&#8217;s IP and put it in nginx.conf
# RUN /root/container_ip.sh

# Reload the damn nginx service
# CMD [&#8220;service&#8221;, &#8220;nginx&#8221;, &#8220;restart&#8221;]
CMD /usr/sbin/nginx -g &#8220;daemon off;&#8221;
&#8220;`

Not much to explain here until I get to the part about deploying to AWS. Here&#8217;s nginx.conf -:

&#8220;` sh nginx.conf
worker_processes 1;

events {
    worker_connections 1024;
}

http {

    server {
        listen 80;
        server_name example.org;

        access_log /dev/stdout;
        error_log /dev/stdout info;

        location /static/ {
            alias /usr/src/app/repo/static;
        }

        location /static/javascripts/ {
          default_type text/javascript;
          alias /usr/src/app/repo/static/javascripts/;
        }

        location /static/stylesheets/ {
          default_type text/css;
          alias /usr/src/app/repo/static/stylesheets/;
        }

	location /static/bower_components/ {
	  types {
	    text/css css;
	    text/javascript js;
	  }
	  alias /usr/src/app/repo/static/bower_components/;
	}

	location /static/partials/ {
	  types {
	    text/html html;
	  }
	  alias /usr/src/app/repo/static/partials/;
	}

	location /static/admin/ {
          alias /usr/src/app/repo/static/admin/;
	}

	location /static/admin/css {
	  default_type text/css;
	  alias /usr/src/app/repo/static/admin/css;
	}

	location /static/admin/js {
	  default_type text/javascript;
	  alias /usr/src/app/repo/static/admin/js;
	}

	location /static/admin/img {
	  types {
	    image/png png;
	    image/jpeg jpg;
	  }
	  alias /usr/src/app/repo/static/admin/img;
	}

        location / {
            proxy_pass http://django:8000;
            proxy_set_header   Host $host;
            proxy_set_header   X-Real-IP $remote_addr;
            proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header   X-Forwarded-Host $server_name;
        }
    }
}
&#8220;`

As you can see, I had to make it very thorough about what files nginx was going to serve,
from where, and what type mappings would the files have had. The latter had me stumped for
a bit - I got nginx to serve all static files, but it was as if the browser didn&#8217;t know
what to do with them, so take note. Finally, the location directive tells nginx whom
to proxy pass (django container service). There will be a minor change to this bit later
when we deploy on AWS.

The second part of this post will deal with the actual commands needed to deploy the setup;
first, on my local machine (virtualbox driver), and then on AWS. I&#8217;ll add a bonus command
to deploy to DigitalOcean for shits and giggles.
Coming soon!

<div id="discourse-comments"></div>
<script type="text/javascript">
  var discourseUrl = "",
      discourseEmbedUrl = "http://PritishC.github.io/blog/2015/09/03/docker-is-awesome/";

  (function() {
    var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
      d.src = discourseUrl + 'javascripts/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
  })();
</script>
]]></content>
  </entry>
  
</feed>
