<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Devops | Pritish Chakraborty]]></title>
  <link href="http://PritishC.github.io/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://PritishC.github.io/"/>
  <updated>2016-02-14T21:01:33+05:30</updated>
  <id>http://PritishC.github.io/</id>
  <author>
    <name><![CDATA[Pritish Chakraborty]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Docker Is Awesome - Part II]]></title>
    <link href="http://PritishC.github.io/blog/2015/09/04/docker-is-awesome-part-ii/"/>
    <updated>2015-09-04T22:56:15+05:30</updated>
    <id>http://PritishC.github.io/blog/2015/09/04/docker-is-awesome-part-ii</id>
    <content type="html"><![CDATA[This is Part 2 of the 'Docker Is Awesome' mini-series. You can catch Part 1
over [here]({% post_url 2015-09-03-docker-is-awesome %}).

In this article, I'll explain how to use docker-machine, docker-compose and
docker-engine to deploy your setup on your local machine as well as to AWS.

<!--more-->

It is imperative to first understand what this trinity of tools does. docker-engine is the basic component - it is used to create, manipulate and delete
one-off containers. On the command line, one can invoke it by typing just
`docker`.

docker-compose is a tool built on top of docker-engine which allows us to
run multi-container systems. In our setup, we have three services - django,
postgres and nginx. Each runs in their own container, and compose lets us
coordinate them. The docker-compose reference urges us not to use it in
production *yet*, but it&#8217;s pretty good, so screw that!

docker-machine is a tool used to create and provision Docker hosts. This
provisioning can be done either on your local machine or on a cloud provider;
it supports quite a few of them, including AWS and DigitalOcean.

Here are the contents of the `rebuild_docker.sh` file -:

&#8220;` bash rebuild_docker.sh
docker-compose build
docker-compose up -d
docker-compose ps
&#8220;`

The `build` option builds each service specified in the `docker-compose.yml` file.
The `up -d` option creates and starts the containers given in that list. Finally,
`ps` lets us see each running container, what command they&#8217;re running and what
their status is (exit codes if there were any errors).

With all that out of the way, let us begin!

## Local Machine Setup

Let&#8217;s create a docker-machine host for our local machine. We will be using the Virtualbox
driver to get that running.

&#8220;` bash docker-machine local
$ docker-machine create -d virtulabox dev;
Creating VirtualBox VM&#8230;
Creating SSH key&#8230;
Starting VirtualBox VM&#8230;
Starting VM&#8230;
To see how to connect Docker to this machine, run: docker-machine env dev
&#8220;`

Well, since it tells us to -:

&#8220;` bash docker-machine local
$ docker-machine env dev
export DOCKER_TLS_VERIFY=&#8221;1&#8221;
export DOCKER_HOST=&#8221;tcp://192.168.99.100:2376&#8221;
export DOCKER_CERT_PATH=&#8221;/home/pritishc/.docker/machine/machines/dev&#8221;
export DOCKER_MACHINE_NAME=&#8221;dev&#8221;
# Run this command to configure your shell: 
# eval &#8220;$(docker-machine env dev)&#8221;
&#8220;`

A bunch of environment variables, hrm. Run the `eval` command as given to set those up.
Now we have a running docker host on our local. Check out the ip of the machine since
you will be needing that later -:

&#8220;` bash docker-machine local
$ docker-machine ip dev
192.168.99.100
&#8220;`

Now we can run the contents of `rebuild_docker.sh` to build, create/start and list our
containers in one go. Note that the building process is initially quite slow; especially
on your local machine. The results of this build will be cached, making each subsequent
build much faster. However, **if you tinker with the Dockerfile of a service (like django),
the lines after the line you tinkered with will not be cached, and will be built again**.

Here&#8217;s what it looks like when the builds are cached -:

&#8220;` bash docker-machine local cached build
postgres uses an image, skipping
Building django&#8230;
Step 0 : FROM ubuntu:14.04
 &#8212;> 91e54dfb1179
Step 1 : ENV DJANGO_CONFIGURATION Docker
 &#8212;> Using cache
 &#8212;> 6c23847d177e
Step 2 : RUN apt-get update
 &#8212;> Using cache
 &#8212;> 37ceafd99c8b
Step 3 : RUN apt-get install -y ca-certificates git-core ssh nodejs npm python-pip libpq-dev python-dev
 &#8212;> Using cache
 &#8212;> ca6793a2add5
Step 4 : RUN ln -s /usr/bin/nodejs /usr/bin/node
 &#8212;> Using cache
 &#8212;> f9311c741b62
Step 5 : ENV HOME /root
 &#8212;> Using cache
 &#8212;> b0f6dbd6a6fa
Step 6 : ADD ssh/ /root/.ssh/
 &#8212;> Using cache
 &#8212;> 1a50ee500dba
Step 7 : RUN chmod 600 /root/.ssh/*
 &#8212;> Using cache
 &#8212;> 475bf637fea7
Step 8 : RUN ssh-keyscan bitbucket.org > /root/.ssh/known_hosts
 &#8212;> Using cache
 &#8212;> 1bc79f8a33a0
Step 9 : WORKDIR /usr/src/app
 &#8212;> Using cache
 &#8212;> 6922edc704da
Step 10 : RUN git clone git@bitbucket.org:me/repo.git
 &#8212;> Using cache
 &#8212;> 29160d78c443
Step 11 : WORKDIR /usr/src/app/repo
 &#8212;> Using cache
 &#8212;> ca03b1e414cc
Step 12 : RUN pip install -r requirements.txt
 &#8212;> Using cache
 &#8212;> 56fb3e85c03c
Step 13 : RUN npm install -g bower
 &#8212;> Using cache
 &#8212;> e5660af6e528
Step 14 : RUN bower &#8211;allow-root install
 &#8212;> Using cache
 &#8212;> d8b0d8cf33e6
Step 15 : ENV AWS_ACCESS_KEY youcanttouchthis
 &#8212;> Using cache
 &#8212;> 057f430aa52d
Step 16 : ENV AWS_SECRET_ACCESS_KEY secretsauce
 &#8212;> Using cache
 &#8212;> 68291ffdbd6c
Step 17 : ENV S3_BUCKET_NAME datbuckettho
 &#8212;> Using cache
 &#8212;> 2ba6884067d2
Step 18 : ENV DB_NAME postgres
 &#8212;> Using cache
 &#8212;> 951c81446ee1
Step 19 : ENV DB_USER postgres
 &#8212;> Using cache
 &#8212;> b85ead2a75e3
Step 20 : ENV DB_PASS postgres
 &#8212;> Using cache
 &#8212;> 475c43341026
Step 21 : ENV DB_SERVICE postgres
 &#8212;> Using cache
 &#8212;> 4cc910cd2a51
Step 22 : ADD ./static/admin /usr/src/app/repo/static/admin
 &#8212;> Using cache
 &#8212;> d70e3dd5c885
Step 23 : WORKDIR /usr/src/app/repo/project
 &#8212;> Using cache
 &#8212;> 255623db8059
Step 24 : CMD gunicorn project.wsgi -w 2 -b 0.0.0.0:8000 &#8211;log-level -
 &#8212;> Using cache
 &#8212;> 55a7c489cb3e
Successfully built 55a7c489cb3e
Building nginx&#8230;
Step 0 : FROM nginx
 &#8212;> cd3cf76a61ee
Step 1 : MAINTAINER Pritish Chakraborty
 &#8212;> Using cache
 &#8212;> 73affa8810c3
Step 2 : COPY nginx.conf /etc/nginx/nginx.conf
 &#8212;> Using cache
 &#8212;> 3b73ef0ef3e8
Step 3 : COPY container_ip.sh /root/container_ip.sh
 &#8212;> Using cache
 &#8212;> b3fbe36b0674
Step 4 : RUN /root/container_ip.sh
 &#8212;> Using cache
 &#8212;> 5df4fe6c5403
Step 5 : CMD service nginx restart
 &#8212;> Running in 642215ba03c6
 &#8212;> 4d2df9239cb3
Removing intermediate container 642215ba03c6
Step 6 : CMD /usr/sbin/nginx -g &#8220;daemon off;&#8221;
 &#8212;> Running in 5bdc232c1761
 &#8212;> 2d55cf5df93b
Removing intermediate container 5bdc232c1761
Successfully built 2d55cf5df93b
projectdocker_postgres_1 is up-to-date
projectdocker_django_1 is up-to-date
Recreating projectdocker_nginx_1&#8230;
      Name             Command             State              Ports       
&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;-
projectdocker_dja   gunicorn           Up                 8000/tcp         
ngo_1              project.wsgi -w 2                                       
                   &#8230;                                                    
projectdocker_ngi   /bin/sh -c         Up                 443/tcp, 0.0.0.0 
nx_1               /usr/sbin/nginx                       :80->80/tcp      
                   &#8230;                                                    
projectdocker_pos   /docker-           Up                 5432/tcp         
tgres_1            entrypoint.sh                                          
                   postgres
&#8220;`

Pretty neat, huh? You can browse to the ip given by docker-machine in your
browser and check your website out in all its glory!

A couple more things - suppose some service breaks unexpectedly. You can
check out the logs using -:

&#8220;` bash docker-compose logs
$ docker-compose logs <servicename>
&#8220;`

If you want to check out the contents of your container, run -:

&#8220;` bash docker-compose run
$ docker-compose run <servicename> bash
&#8220;`

## AWS Setup

This is it. The biggie. This is the part where you get to see your app running
on the cloud. We&#8217;re going to deploy our multi-container setup to Amazon EC2. But
first, we&#8217;re going to have to prepare a few things.

* Create a Virtual Private Cloud instance (VPC) which your EC2 instance will use.
This can be done from the VPC Management Console. If you signed up on the free-tier,
you might have a VPC instance running from the beginning. If not, create one!
Here&#8217;s what it looks like -:

{% img /images/AWS_VPC.png &#8216;AWS VPC&#8217; %}

{% img /images/AWS_VPC_Subnets.png &#8216;AWS VPC Subnets&#8217; %}

When creating a VPC, keep the tenancy as &#8216;default&#8217; and an example of a CIDR block
that you could choose is 172.31.0.0/16, where /16 is the subnet mask. Amazon does
the rest for you.

* With the VPC and its subnets (if there are none, create a public subnet) created,
we need to assign some security groups. First, according to Amazon&#8217;s [documentation](http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html),
make a security group (call it whatever you like) and add the rules as follows -:

{% img /images/AWS_SG_1.png &#8216;AWS Security Group Inbound Rules&#8217; %}

{% img /images/AWS_SG_2.png &#8216;AWS Security Group Outbound Rules&#8217; %}

In the inbound rules, IP ranges that I pixelized are my own. In these specific rules,
you&#8217;d select &#8216;My Own IP&#8217; when creating them. Finally, add the security group to your
VPC.

* With the VPC set up, go to the Identity Access Management console (IAM), and
generate a pair of credentials for docker-machine to use when setting up your EC2
box. I wasn&#8217;t able to figure out the security groups that this set of IAM credentials
should&#8217;ve had, so I gave it FullAdministratorAccess - this is usually not advisable.
Also note that the set of credentials which I put in my django Dockerfile are **not**
the same as these.

Now we can provision our docker host on AWS. Do it with this command -:

&#8220;` bash docker-machine AWS
docker-machine create \    
&#8211;driver amazonec2 \
&#8211;amazonec2-access-key nicekeym8 \
&#8211;amazonec2-secret-key oososecret \
&#8211;amazonec2-vpc-id vpc-id \
&#8211;amazonec2-region some-region \
&#8211;amazonec2-zone zone-letter \
ec2box
&#8220;`

You can get the information from the VPC management console. This command takes a
while to execute, but when it is done, you&#8217;ll have to run the `eval` command like
we did for our local setup, to point docker-machine to our AWS host.

Once the machine is provisioned, go check it out on your EC2 management console -:

{% img /images/AWS_EC2.png &#8216;AWS EC2&#8217; %}

There&#8217;s still a bit of security group editing to do. Now when you check out your
VPC, you will find that it has a new group called `docker-machine`. Edit this group
and add the following inbound rule -:

{% img /images/AWS_VPC_dockermachine.png &#8216;docker-machine Rules&#8217; %}

Add this TCP rule for your own IP.

Now you can browse to the ip given by `docker-machine ip ec2box`/the one listed on
your EC2 instances page, and examine your webapp on the cloud at leisure!
Pat yourself on the back, you did it. Or not&#8230;

## What About Nginx?

Ah, so you *did* read my previous article carefully. If I&#8217;m right, you&#8217;ll find
that nginx is giving you weird errors (most likely a 502).

The last bit of trickery that I added to the nginx Dockerfile revolves around this
file - `container_ip.sh`. Its contents are as follows -:

&#8220;` bash container_ip.sh
# The awk command gets the IP for the django container from /etc/hosts
# The sed command replaces the placeholder in nginx conf with this IP
# Finally, the config placeholder is replaced with the IP
ip=`awk &#8216;/django/ {print $1; exit}&#8217; /etc/hosts`; sed -i &#8220;s/{{container_ip}}/$ip/g&#8221; /etc/nginx/nginx.conf
&#8220;`

And the said placeholder is put in like this -:

{% raw %}
&#8220;` bash nginx config
location / {
    proxy_pass http://{{container_ip}}:8000;
    proxy_set_header   Host $host;
    proxy_set_header   X-Real-IP $remote_addr;
    proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header   X-Forwarded-Host $server_name;
}
&#8220;`

`{{container_ip}}`{% endraw %} is in place of &#8216;django&#8217;, which is replaced by the actual IP given in
the `/etc/hosts` file of the nginx container - you can confirm this for yourself by running
bash on the nginx container and running the awk and sed commands.

You might want to put `container_ip.sh` in the nginx directory so that docker can
find it when performing the ADD operation.

And now, run the following commands to recreate the nginx container -:

&#8220;` bash nginx recreation
$ docker-compose build &#8211;no-cache nginx
$ docker-compose up -d
&#8220;`

And there you have it. Give it a bit of time for AWS to iron everything out, and with
luck, you&#8217;ll be able to play with your webapp on the cloud.

Hope you enjoyed this article :) I&#8217;ll write another one soon for that django-storages +
Amazon S3 setup of mine.

<div id="discourse-comments"></div>
<script type="text/javascript">
  var discourseUrl = "",
      discourseEmbedUrl = "http://PritishC.github.io/blog/2015/09/04/docker-is-awesome-part-ii/";

  (function() {
    var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
      d.src = discourseUrl + 'javascripts/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
  })();
</script>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Is Awesome - Part I]]></title>
    <link href="http://PritishC.github.io/blog/2015/09/03/docker-is-awesome/"/>
    <updated>2015-09-03T21:56:16+05:30</updated>
    <id>http://PritishC.github.io/blog/2015/09/03/docker-is-awesome</id>
    <content type="html"><![CDATA[It's been a while since I've written a blog post. There's been lots of changes that I've
had to deal with, the big one being a change of workplace. And I realize that this post
should've been the one explaining my adventures with Elasticsearch. That will have to wait
for a bit, my apologies!

In this article, I'm going to explain my (possibly flawed) method of deploying a Django/Angular
app using Docker. I've wanted to learn how to deploy an app to AWS for a while, and Docker
helped me do just that.

<!--more-->

I assume that the reader has a basic knowledge of the docker toolbox; if not, I&#8217;ll explain them in brief in
part 2. To begin with, here&#8217;s the directory structure of my app for reference (generated using the
lovely `tree` command on Linux) -:

&#8220;` bash app structure
|&#8211; bower.json
|&#8211; CONTRIBUTORS
|&#8211; project
|   |&#8211; project
|   |   |&#8211; aws_settings.py (django-storages credentials here)
|   |   |&#8211; heroku_settings.py
|   |   |&#8211; __init__.py
|   |   |&#8211; settings.py
|   |   |&#8211; urls.py
|   |   |&#8211; views.py
|   |   `&#8211; wsgi.py
|   |&#8211; manage.py
|   |&#8211; app1
|   |   |&#8211; __init__.py
|   |   |&#8211; permissions.py
|   |   |&#8211; serializers.py
|   |   |&#8211; services.py
|   |   |&#8211; tests.py
|   |   `&#8211; views.py
|   `&#8211; users
|       |&#8211; __init__.py
|       |&#8211; models.py
|       |&#8211; permissions.py
|       |&#8211; serializers.py
|       `&#8211; views.py
|&#8211; gulpfile.js
|&#8211; package.json
|&#8211; Procfile
|&#8211; README.md
|&#8211; requirements.txt
|&#8211; scripts
|   `&#8211; postInstall.sh
|&#8211; static
|   |&#8211; javascripts
|   |   |&#8211; app.js
|   |   |&#8211; controllers
|   |   |   `&#8211; controllers.js
|   |   |&#8211; directives
|   |   |   `&#8211; directives.js
|   |   `&#8211; services
|   |       `&#8211; services.js
|   |&#8211; partials (angular view stuff)
|   `&#8211; stylesheets
|       `&#8211; styles.css
|&#8211; templates
|   |&#8211; index.html
|   |&#8211; javascripts.html
|   |&#8211; navbar.html
|   `&#8211; stylesheets.html
&#8220;`

This follows from the nice boilerplate provided by [thinkster.io](https://github.com/brwr/thinkster-django-angular-boilerplate) - take a look at their nifty tutorials [here](http://thinkster.io).

I first stumbled across this article on [realpython.com](https://realpython.com/blog/python/django-development-with-docker-compose-and-machine/), which greatly piqued my interest. I realized that docker had
moved on to becoming a suite of tools - docker itself becoming docker-engine in name. However,
I faced issues (being a total n00b in devops) in setting up with the configuration that they
specified, so I decided to go for something simpler. I found just what I needed in [Andre&#8217;s](https://github.com/andrecp/django-tutorial-docker-nginx-postgres)
tutorial for deploying a simple Docker-Nginx-Django-Postgres setup.

One thing I noticed in both configurations is that the code repository was bundled alongwith the docker
stuff (needed by docker-compose) for production.
I couldn&#8217;t agree with that, so I decided to look up a method to clone my repository while creating
the docker container. I then kept my code and deployment repositories separate, and found that Bitbucket
(where my repositories are hosted) have a feature called deployment keys - SSH keys that have read-only
access to a repository. This was exactly what I needed.

Here is the directory structure of my docker deployment repository -:

&#8220;` bash docker directories
|&#8211; docker-compose.yml
|&#8211; Dockerfile
|&#8211; ec2box.sh
|&#8211; nginx
|   |&#8211; container_ip.sh
|   |&#8211; Dockerfile
|   `&#8211; nginx.conf
|&#8211; README.md
|&#8211; rebuild_docker.sh
|&#8211; ssh
|   |&#8211; bb_deploy.rsa
|   |&#8211; bb_deploy.rsa.pub
|   `&#8211; config
`&#8211; static
    `&#8211; admin
&#8220;`

* docker-compose.yml : The file that controls how docker-compose builds docker containers and runs them
* Dockerfile : The Dockerfile for the django/angular container
* ec2box.sh : A small script containing a single command which creates the whole setup using AWS drivers
* nginx - Directory containing specifics for the nginx container, where container_ip.sh is another
small script which I needed when deploying on AWS
* rebuild_docker.sh : A script from Andre&#8217;s repository for quickly build-and-up containers using docker-compose
* ssh : Directory containing my Bitbucket deployment key and a SSH config file
* static : Django admin static files

The contents of docker-compose.yml are as follows -:

&#8220;` yaml docker-compose.yml
# Nginx
nginx:
    build: ./nginx
    volumes_from:
        - django
    links:
        - django
    ports:
        - &#8220;80:80&#8221;

# This defines a service for the Django app
# Will include the Angular frontend
django:
    build: .
    volumes:
        - .:/root
        - /usr/src/app
    expose:
        - &#8220;8000&#8221;
    links:
        - postgres

# This defines a service for the Postgres database
postgres:
    image: postgres:latest
&#8220;`

Note how the nginx container definition specifies a `volumes_from` section, of which the django
container is a part. The host port 80 has been mapped to container port 80, as nginx requires.
One little caveat: make sure that no other docker containers are hogging up port 80, because you
will have a painful time trying to find out why your nginx container keeps dying on you. The `links`
directive creates entries in the nginx container&#8217;s `/etc/hosts` file for the django container&#8217;s IP/hostname.
This will come in use later when we deploy to AWS.

The django container definition has a few small differences from the one mentioned at realpython or
Andre&#8217;s tutorial. We mount the volume on `/root` instead of `/usr/src/app`, because the latter does
not exist until we clone the code repository. Additionally, we expose `/usr/src/app` as a volume, so
that the nginx container does not run into a load of 404s when trying to serve static files. Port 8000
is exposed as we shall be running gunicorn on that port, and there is a link to the postgres container.

The postgres definition is not much to talk about, as it is built from an image from the docker registry
(yes, they have a registry of known docker containers!).


Let&#8217;s take a look at the Dockerfile for the django container -:

&#8220;` sh Dockerfile (django)
FROM ubuntu:14.04

ENV DJANGO_CONFIGURATION Docker

# First, we need to get git, and clone our repository
# Additionally, get everything else here too, such as nodejs and npm

RUN apt-get update
RUN apt-get install -y ca-certificates git-core ssh nodejs npm python-pip libpq-dev python-dev
RUN ln -s /usr/bin/nodejs /usr/bin/node

ENV HOME /root

# Add custom ssh keypair - usually Bitbucket deployment keys
ADD ssh/ /root/.ssh/

# Fix permissions
RUN chmod 600 /root/.ssh/*

# Avoid first connection host confirmation
RUN ssh-keyscan bitbucket.org > /root/.ssh/known_hosts

# Clone the repo
WORKDIR /usr/src/app
RUN git clone git@bitbucket.org:username/repo

# Install requirements
WORKDIR /usr/src/app/repo
RUN pip install -r requirements.txt
RUN npm install -g bower
RUN bower &#8211;allow-root install

# S3 Storage for django-storages
ENV AWS_ACCESS_KEY yourkeyhere
ENV AWS_SECRET_ACCESS_KEY yoursecretsaucehere
ENV S3_BUCKET_NAME yourbucketnamehere

# DB Settings
ENV DB_NAME postgres
ENV DB_USER postgres
ENV DB_PASS postgres
ENV DB_SERVICE postgres

# Add Django Admin CSS
ADD ./static/admin /usr/src/app/repo/static/admin

WORKDIR /usr/src/app/repo/defsec
CMD [&#8220;gunicorn&#8221;, &#8220;app.wsgi&#8221;, &#8220;-w&#8221;, &#8220;2&#8221;, &#8220;-b&#8221;, &#8220;0.0.0.0:8000&#8221;, &#8220;&#8211;log-level&#8221;, &#8220;-&#8220;]
&#8220;`

The `DJANGO_CONFIGURATION` environment variable is used in django project settings. First, we
install all the necessary command line tools, and create a symbolic link so that nodejs plays
well. We then add our custom keypair to the `.ssh` directory, and run a permissions fix command.
This will allow us to clone the code repository. Before running the clone command, we run a
`ssh-keyscan` so that the cloning process is automatic - no key passphrase prompts. Some may argue
that this lowers security, but that is a topic for another post altogether.
After cloning, I install all django/angular requirements using pip and bower. A few more environment
variables are then set, first for django-storages (a topic for a soon-to-come post: how I set up
image uploads to Amazon S3 with django-storages) and then for the postgres database. Finally,
django admin static files are added (yes, these don&#8217;t come out of nowhere, they need to be added
for nginx to serve them) and run the gunicorn server.

With that covered, let&#8217;s check out the contents of the nginx directory. First up is the nginx
Dockerfile -:

&#8220;` sh nginx Dockerfile
# Set nginx base image
FROM nginx

# File Author / Maintainer
MAINTAINER Pritish Chakraborty

# Copy custom configuration file from the current directory
COPY nginx.conf /etc/nginx/nginx.conf

# Uncomment the commented Dockerfile lines below when pushing to AWS
# COPY container_ip.sh /root/container_ip.sh

# Get django container&#8217;s IP and put it in nginx.conf
# RUN /root/container_ip.sh

# Reload the damn nginx service
# CMD [&#8220;service&#8221;, &#8220;nginx&#8221;, &#8220;restart&#8221;]
CMD /usr/sbin/nginx -g &#8220;daemon off;&#8221;
&#8220;`

Not much to explain here until I get to the part about deploying to AWS. Here&#8217;s nginx.conf -:

&#8220;` sh nginx.conf
worker_processes 1;

events {
    worker_connections 1024;
}

http {

    server {
        listen 80;
        server_name example.org;

        access_log /dev/stdout;
        error_log /dev/stdout info;

        location /static/ {
            alias /usr/src/app/repo/static;
        }

        location /static/javascripts/ {
          default_type text/javascript;
          alias /usr/src/app/repo/static/javascripts/;
        }

        location /static/stylesheets/ {
          default_type text/css;
          alias /usr/src/app/repo/static/stylesheets/;
        }

	location /static/bower_components/ {
	  types {
	    text/css css;
	    text/javascript js;
	  }
	  alias /usr/src/app/repo/static/bower_components/;
	}

	location /static/partials/ {
	  types {
	    text/html html;
	  }
	  alias /usr/src/app/repo/static/partials/;
	}

	location /static/admin/ {
          alias /usr/src/app/repo/static/admin/;
	}

	location /static/admin/css {
	  default_type text/css;
	  alias /usr/src/app/repo/static/admin/css;
	}

	location /static/admin/js {
	  default_type text/javascript;
	  alias /usr/src/app/repo/static/admin/js;
	}

	location /static/admin/img {
	  types {
	    image/png png;
	    image/jpeg jpg;
	  }
	  alias /usr/src/app/repo/static/admin/img;
	}

        location / {
            proxy_pass http://django:8000;
            proxy_set_header   Host $host;
            proxy_set_header   X-Real-IP $remote_addr;
            proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header   X-Forwarded-Host $server_name;
        }
    }
}
&#8220;`

As you can see, I had to make it very thorough about what files nginx was going to serve,
from where, and what type mappings would the files have had. The latter had me stumped for
a bit - I got nginx to serve all static files, but it was as if the browser didn&#8217;t know
what to do with them, so take note. Finally, the location directive tells nginx whom
to proxy pass (django container service). There will be a minor change to this bit later
when we deploy on AWS.

The second part of this post will deal with the actual commands needed to deploy the setup;
first, on my local machine (virtualbox driver), and then on AWS. I&#8217;ll add a bonus command
to deploy to DigitalOcean for shits and giggles.
Coming soon!

<div id="discourse-comments"></div>
<script type="text/javascript">
  var discourseUrl = "",
      discourseEmbedUrl = "http://PritishC.github.io/blog/2015/09/03/docker-is-awesome/";

  (function() {
    var d = document.createElement('script'); d.type = 'text/javascript'; d.async = true;
      d.src = discourseUrl + 'javascripts/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(d);
  })();
</script>
]]></content>
  </entry>
  
</feed>
